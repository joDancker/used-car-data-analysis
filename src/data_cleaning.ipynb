{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data cleaning of used car data set.\"\"\"\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "path_to_dataset = \"../data/used_car_dataset.csv\"\n",
    "\n",
    "used_car_data = pd.read_csv(\n",
    "    path_to_dataset,\n",
    "    dtype={\n",
    "        \"publication_history\": str,\n",
    "        \"price_sek\": int,\n",
    "        \"price_history\": str,\n",
    "        \"entry_year\": int,\n",
    "        \"mileage_km\": int,\n",
    "    },\n",
    "    parse_dates=[\"publication_datetime\"],\n",
    "    infer_datetime_format=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "used_car_data = used_car_data.iloc[:, 1:]\n",
    "\n",
    "used_car_data.reset_index(drop=True, inplace=True)\n",
    "used_car_data = used_car_data.fillna(value=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General characteristics of data set\n",
    "\n",
    "Determine general characteristics to get an idea about the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(used_car_data.shape)\n",
    "used_car_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Get an idea about number of unique and missing values\n",
    "\n",
    "I use this information later to decide which columns to keep and which columns need to be looked at to substitute with data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"unique_values\": used_car_data.nunique(axis=0),\n",
    "        \"number_missing_values\": used_car_data.isna().sum(),\n",
    "        \"percentage_missing_values\": used_car_data.isna().sum()\n",
    "        / used_car_data.shape[0]\n",
    "        * 100,\n",
    "        \"data_type\": used_car_data.dtypes,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key parameters of numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the data set are written in small or capital letters. To align their writting for the data cleaning, I change all writting to small letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "After I have taken a superficial look at the data set, I will start with the actual data cleaning. \n",
    "\n",
    "I will begin with \n",
    "- removing duplicates,Â \n",
    "- correcting syntax errors (e.g., remove trailing white spaces and fix typos (using map, pattern match or fuzzy matching), etc.),\n",
    "- unifying missing values to NA (e.g., they can be \"0\", \"None\", \"INF\", etc.),\n",
    "- standardizing values (e.g., convert strings to lower case letters, convert numeric values of the same variable to same measurement unit).\n",
    "\n",
    "This will focus on \n",
    "1) reduce the number of different car models,\n",
    "2) handle missing values,\n",
    "3) remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_data.drop_duplicates(subset=\"url\", inplace=True)\n",
    "used_car_data.sort_values(by=\"publication_datetime\", inplace=True)\n",
    "\n",
    "used_car_data = used_car_data.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "used_car_data[\"price_sek\"] = used_car_data[\"price_sek\"].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reduce the number od different car models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some model names do also include the manufacturer name. To align all model names I remove the manufacturer name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for manufacturer in used_car_data[\"manufacturer\"].unique():\n",
    "    used_car_data[\"model\"].loc[used_car_data[\"manufacturer\"] == manufacturer].replace(\n",
    "        manufacturer, \"\", inplace=True\n",
    "    )\n",
    "\n",
    "used_car_data[\"model\"] = used_car_data[\"model\"].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some model names are empty but notes have a potential name. Hence, I extract the model name from the first substring of the note string. Here, I assume that each model has a name formed by a single string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = used_car_data.loc[used_car_data[\"model\"] == \"\", \"note\"].str.split().tolist()\n",
    "used_car_data.loc[used_car_data[\"model\"] == \"\", \"model\"] = [part[0] for part in model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The assumption is faulty when we have model names consisting of a a number and a letter which are separeted by a white space. Hence, I checked manually for cars / manufacturers with these errors.\n",
    "\n",
    "Probably there are more issues with the model name but the naming convention is not uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some BMWs the model contains only the number missing the letter\n",
    "model_notes = used_car_data.loc[\n",
    "    used_car_data[\"manufacturer\"] == \"bmw\", \"note\"\n",
    "].str.split()\n",
    "\n",
    "for idx, note in model_notes.iteritems():\n",
    "    if note is not np.nan:\n",
    "        # only models with three numbers have an additional letter\n",
    "        if len(note[0]) == 1 and len(used_car_data.loc[idx, \"model\"]) == 3:\n",
    "            used_car_data.loc[idx, \"model\"] = (\n",
    "                used_car_data.loc[idx, \"model\"] + \"\" + note[0]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some Volvos miss the addition \"cross country\"\n",
    "model_notes = used_car_data.loc[\n",
    "    used_car_data[\"manufacturer\"] == \"volvo\", \"note\"\n",
    "].str.split()\n",
    "for idx, note in model_notes.iteritems():\n",
    "    if note is not np.nan:\n",
    "        if (\"cross country\" in model) and used_car_data.loc[idx, \"model\"].isin(\n",
    "            \"cross country\"\n",
    "        ):\n",
    "            used_car_data.loc[idx, \"model\"] = (\n",
    "                used_car_data.loc[idx, \"model\"] + \" cross country\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group similar model names \n",
    "\n",
    "As there is not uniform naming convention often the same car model is written differently. \n",
    "Hence, I group model names based on a fuzzy-logic. \n",
    "\n",
    "The thresholds of the fuzzy logic were set manually and derived from a detailed analysis. However, this method is a bit error prone and strongly depends on the chosen thresholds. \n",
    "\n",
    "The used thresholds where determined based on an analysis with different manufacturers. Nevertheless, some matches are not found and some are put together wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_matches_of_model_name(unique_model_names, fuzz_scorer, threshold):\n",
    "    \"\"\"Score the matches between model names.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unique_model_names : list\n",
    "        name of car model\n",
    "    fuzz_scorer : str\n",
    "        fuzzywuzzy scorer\n",
    "    threshold : int\n",
    "        threshold to decide model are similar\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    similarity : DataFrame\n",
    "        similarity score for each model\n",
    "    \"\"\"\n",
    "    score = [\n",
    "        (x,) + i\n",
    "        for x in unique_model_names\n",
    "        for i in process.extract(x, unique_model_names, scorer=fuzz_scorer)\n",
    "    ]\n",
    "    similarity = pd.DataFrame(score, columns=[\"model\", \"match\", \"score\"])\n",
    "    # pick only matches that score above threshold and are not a self-match\n",
    "    similarity = similarity[\n",
    "        (similarity[\"score\"] >= threshold)\n",
    "        & (similarity[\"model\"] != similarity[\"match\"])\n",
    "    ]\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def find_and_replace_similar_models(\n",
    "    manufacturer, scoring_threshold, replacing_threshold, dataset\n",
    "):\n",
    "    \"\"\"Find matches of similar models and replace them.\n",
    "\n",
    "    The similarity is matched based on two scores as together they provide a better\n",
    "    matching results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    manufacturer : str\n",
    "        name of manufacturer\n",
    "    scoring_threshold : int\n",
    "        threshold of similarity score to be picked as potential replacement\n",
    "    replacing_threshold : int\n",
    "        threshold of similarity score to replace model name with match\n",
    "    dataset: DataFrame\n",
    "        dataset of used cars\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataset: DataFrame\n",
    "        updated dataset of used cars\n",
    "    \"\"\"\n",
    "    unique_model_names = (\n",
    "        dataset.loc[dataset[\"manufacturer\"] == manufacturer, \"model\"].unique().tolist()\n",
    "    )\n",
    "\n",
    "    # match similar models based on two scores as sometimes one finds a\n",
    "    # good match while the other does not\n",
    "    similarity_sort_ratio = score_matches_of_model_name(\n",
    "        unique_model_names, fuzz.token_sort_ratio, scoring_threshold\n",
    "    )\n",
    "    similarity_set_ratio = score_matches_of_model_name(\n",
    "        unique_model_names, fuzz.token_set_ratio, scoring_threshold\n",
    "    )\n",
    "\n",
    "    # take the one that has the higher score\n",
    "    unique_models = similarity_sort_ratio[\"model\"].unique().tolist()\n",
    "    matches = pd.DataFrame(columns=[\"model\", \"match\", \"score\"])\n",
    "    for model in unique_models:\n",
    "\n",
    "        max_score_sort = similarity_sort_ratio.loc[\n",
    "            similarity_sort_ratio[\"model\"] == model, \"score\"\n",
    "        ].max()\n",
    "        max_score_set = similarity_set_ratio.loc[\n",
    "            similarity_set_ratio[\"model\"] == model, \"score\"\n",
    "        ].max()\n",
    "\n",
    "        if max_score_sort > max_score_set:\n",
    "            idx_max = similarity_sort_ratio.loc[\n",
    "                similarity_sort_ratio[\"model\"] == model, \"score\"\n",
    "            ].idxmax()\n",
    "            matches = pd.concat(\n",
    "                [\n",
    "                    matches,\n",
    "                    pd.DataFrame(\n",
    "                        data={\n",
    "                            \"model\": [model],\n",
    "                            \"match\": [similarity_sort_ratio.loc[idx_max, \"match\"]],\n",
    "                            \"score\": [similarity_sort_ratio.loc[idx_max, \"score\"]],\n",
    "                        }\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        else:\n",
    "            idx_max = similarity_set_ratio.loc[\n",
    "                similarity_set_ratio[\"model\"] == model, \"score\"\n",
    "            ].idxmax()\n",
    "            matches = pd.concat(\n",
    "                [\n",
    "                    matches,\n",
    "                    pd.DataFrame(\n",
    "                        data={\n",
    "                            \"model\": [model],\n",
    "                            \"match\": [similarity_set_ratio.loc[idx_max, \"match\"]],\n",
    "                            \"score\": [similarity_set_ratio.loc[idx_max, \"score\"]],\n",
    "                        }\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    # take shortest model name as name for machting groups\n",
    "    matches[\"chosen_model_name\"] = [\n",
    "        min([model, match], key=len)\n",
    "        for model, match in zip(matches[\"model\"], matches[\"match\"])\n",
    "    ]\n",
    "\n",
    "    # check if matching makes sense\n",
    "    matches.groupby([\"chosen_model_name\", \"score\"]).agg(\n",
    "        {\"model\": \", \".join}\n",
    "    ).sort_values([\"score\"], ascending=False)\n",
    "\n",
    "    matches.drop(\n",
    "        index=matches.index[matches[\"score\"] < replacing_threshold], inplace=True\n",
    "    )\n",
    "\n",
    "    # replace model names with matches\n",
    "    replace_model_names = dict(\n",
    "        zip(matches[\"model\"].tolist(), matches[\"chosen_model_name\"].tolist())\n",
    "    )\n",
    "    dataset.loc[dataset[\"manufacturer\"] == manufacturer, \"model\"] = dataset.loc[\n",
    "        dataset[\"manufacturer\"] == manufacturer, \"model\"\n",
    "    ].replace(replace_model_names)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match similar model names for same manufacturer\n",
    "print(\n",
    "    f\"Number of unique model names before matching: {used_car_data['model'].nunique()}\"\n",
    ")\n",
    "\n",
    "for manufacturer in used_car_data[\"manufacturer\"].unique().tolist():\n",
    "    used_car_data = find_and_replace_similar_models(\n",
    "        manufacturer, 80, 100, used_car_data\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"Number of unique model names after matching: {used_car_data['model'].nunique()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting manufacturer names\n",
    "\n",
    "Some manufacturer names were not correctly split during the web scraping. This relates mostly to manufacturer names which consits of two or more words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_data.loc[\n",
    "    used_car_data[\"manufacturer\"] == \"land\", \"manufacturer\"\n",
    "] = \"land rover\"\n",
    "used_car_data.loc[\n",
    "    used_car_data[\"manufacturer\"] == \"alfa\", \"manufacturer\"\n",
    "] = \"alfa romeo\"\n",
    "used_car_data.loc[used_car_data[\"manufacturer\"] == \"vw\", \"manufacturer\"] = \"volkswagen\"\n",
    "used_car_data[\"manufacturer\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set also contains advertisements which come from a test account, which I will just remove from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_data.drop(\n",
    "    index=used_car_data.index[used_car_data[\"provider\"] == \"Blocket Testkonto 2\"],\n",
    "    inplace=True,\n",
    ")\n",
    "used_car_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Handle missing values\n",
    "\n",
    "The data set contains a lot of missing values for detailed information about the car, such as CO2-emissions, fuel consumption, and dimensions of the car. \n",
    "\n",
    "The missing values are can be categorical, such as the type of drive (autmatic or manual), or numeric , such as CO2-emission. \n",
    "\n",
    "Categorical values are imputed by choosing the category which occurs most often for the same car model. \n",
    "\n",
    "Numerical values are imputed by choosing the median of the same car model if the difference between the mean and median are below a threshold . \n",
    "\n",
    "If the data set does not contain a model to take a value from, then no value is imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_categoricals(dataset, category):\n",
    "    \"\"\"Impute missing categorical values of given category in data set.\n",
    "\n",
    "    Imputation is based on category which appears most often in data set of\n",
    "    the same model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : DataFrame\n",
    "        dataset of used cars\n",
    "    category : str\n",
    "        name of column to be imputed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataset: DataFrame\n",
    "        updated dataset of used cars\n",
    "    \"\"\"\n",
    "    new_category = \"_\".join([\"imputed\", category])\n",
    "    dataset[new_category] = dataset[category]\n",
    "\n",
    "    unique_car_models = dataset.loc[dataset[new_category].isna(), \"model\"].unique()\n",
    "\n",
    "    for model in unique_car_models:\n",
    "        categories_same_model = dataset.loc[\n",
    "            dataset[\"model\"] == model, new_category\n",
    "        ].value_counts()\n",
    "\n",
    "        if len(categories_same_model) > 0:\n",
    "            idx = dataset.loc[dataset[new_category].isna(), \"model\"] == model\n",
    "            dataset.loc[\n",
    "                idx.index[idx].to_list(), new_category\n",
    "            ] = categories_same_model.index[0]\n",
    "\n",
    "    print(\n",
    "        f\"missing values before and after imputing {category}: \",\n",
    "        f\"{dataset[category].isna().sum()} / {dataset[new_category].isna().sum()}\",\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def impute_numerics_based_on_distribution(dataset, category, threshold):\n",
    "    \"\"\"Impute missing numeric values of given category in data set.\n",
    "\n",
    "    The values are imputed by picking a random value within the range of 2 standard\n",
    "    deviations around the mean. However, the value is only imputed if the mean and\n",
    "    median are below a given threshold to ensure that the distribution is close to\n",
    "    symmetry.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : DataFrame\n",
    "        dataset of used cars\n",
    "    category : str\n",
    "        name of column to be imputed\n",
    "    threshold : int\n",
    "        threshold difference between mean and median\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataset: DataFrame\n",
    "        updated dataset of used cars\n",
    "    \"\"\"\n",
    "    new_category = \"_\".join([\"imputed\", category])\n",
    "    dataset[new_category] = dataset[category]\n",
    "\n",
    "    unique_car_models = dataset.loc[dataset[category].isna(), \"model\"].unique()\n",
    "\n",
    "    for model in unique_car_models:\n",
    "\n",
    "        # done so that no warning is printed when there are no values for model type\n",
    "        if dataset.loc[dataset[\"model\"] == model, category].notnull().any():\n",
    "            model_mean = np.nanmean(dataset.loc[dataset[\"model\"] == model, category])\n",
    "            model_median = np.nanmedian(\n",
    "                dataset.loc[dataset[\"model\"] == model, category]\n",
    "            )\n",
    "\n",
    "            if abs(model_mean - model_median) / model_median < threshold:\n",
    "                idx = dataset.loc[dataset[category].isna(), \"model\"] == model\n",
    "\n",
    "                standard_deviation = np.nanstd(\n",
    "                    dataset.loc[dataset[\"model\"] == model, category]\n",
    "                )\n",
    "\n",
    "                dataset.loc[idx.index[idx].to_list(), new_category] = np.random.randint(\n",
    "                    model_mean - 2 * standard_deviation,\n",
    "                    model_mean + 2 * standard_deviation,\n",
    "                    size=len(idx),\n",
    "                )\n",
    "\n",
    "    print(\n",
    "        f\"missing values before and after imputing {category}: \",\n",
    "        f\"{dataset[category].isna().sum()} / {dataset[new_category].isna().sum()}\",\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def impute_numerics_based_on_linear_regression(dataset, category, corr_category):\n",
    "    \"\"\"Impute missing numeric values based on linear regression.\n",
    "\n",
    "    The values are imputed based on a linear correlation to another value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : DataFrame\n",
    "        dataset of used cars\n",
    "    category : str\n",
    "        name of column to be imputed\n",
    "    corr_category : str\n",
    "        name of column / variable which is used for correlation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataset: DataFrame\n",
    "        updated dataset of used cars\n",
    "    \"\"\"\n",
    "    new_category = \"_\".join([\"imputed\", category])\n",
    "    dataset[new_category] = dataset[category]\n",
    "\n",
    "    prediction_dataset = dataset.copy().dropna(axis=0, subset=new_category)\n",
    "    prediction_dataset.dropna(axis=0, subset=corr_category, inplace=True)\n",
    "\n",
    "    x = prediction_dataset[corr_category].to_numpy().reshape((-1, 1))\n",
    "    y = prediction_dataset[new_category].to_numpy()\n",
    "\n",
    "    model = LinearRegression().fit(x, y)\n",
    "\n",
    "    missing_value_idx = dataset.index[\n",
    "        dataset[new_category].isna() & dataset[corr_category].notna()\n",
    "    ]\n",
    "\n",
    "    dataset.loc[missing_value_idx, new_category] = model.predict(\n",
    "        dataset.loc[missing_value_idx, corr_category].to_numpy().reshape((-1, 1))\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"missing values before and after imputing {category}: \",\n",
    "        f\"{dataset[category].isna().sum()} / {dataset[new_category].isna().sum()}\",\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_data = impute_categoricals(used_car_data, \"fuel\")\n",
    "used_car_data = impute_categoricals(used_car_data, \"transmission\")\n",
    "used_car_data = impute_categoricals(used_car_data, \"type_of_drive\")\n",
    "used_car_data = impute_categoricals(used_car_data, \"emission_class\")\n",
    "used_car_data = impute_categoricals(used_car_data, \"car_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\n",
    "    x=\"co2_emission_g/km\",\n",
    "    y=\"fuel_consumption_mixed_l_100km\",\n",
    "    data=used_car_data,\n",
    "    col=\"car_type\",\n",
    ")\n",
    "sns.lmplot(\n",
    "    x=\"co2_emission_g/km\",\n",
    "    y=\"fuel_consumption_highway_l_100km\",\n",
    "    data=used_car_data,\n",
    "    col=\"car_type\",\n",
    ")\n",
    "sns.lmplot(\n",
    "    x=\"fuel_consumption_mixed_l_100km\",\n",
    "    y=\"fuel_consumption_highway_l_100km\",\n",
    "    data=used_car_data,\n",
    "    col=\"car_type\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_data = impute_numerics_based_on_distribution(used_car_data, \"horse_power\", 0.1)\n",
    "used_car_data = impute_numerics_based_on_distribution(\n",
    "    used_car_data, \"fuel_consumption_mixed_l_100km\", 0.1\n",
    ")\n",
    "used_car_data = impute_numerics_based_on_linear_regression(\n",
    "    used_car_data, \"co2_emission_g/km\", \"fuel_consumption_mixed_l_100km\"\n",
    ")\n",
    "used_car_data = impute_numerics_based_on_linear_regression(\n",
    "    used_car_data, \"fuel_consumption_highway_l_100km\", \"fuel_consumption_mixed_l_100km\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Remove outliers \n",
    "\n",
    "As can be seen, the data set has a great variety of the cars' entry years, prices and mileage. \n",
    "\n",
    "For example, the data set seems to \n",
    "- include some oldtimers from the 1960s and earlier \n",
    "- cars which seem very expensive (greater 5 mio. swedish crown\n",
    "- cars with an unreasonably high mileage (greater 300000 km)\n",
    "\n",
    "As I want to focus on cars which are a \"realistic\" option to buy for someone who is looking for a used car to use daily I will remove these \"outliers\" of the overall data set. \n",
    "\n",
    "To identify the outliers I will determine a lower and upper bound based on the interquartile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10, 3))\n",
    "ax[0].boxplot(used_car_data[\"entry_year\"])\n",
    "ax[0].set_title(\"entry year\")\n",
    "ax[1].boxplot(used_car_data[\"price_sek\"])\n",
    "ax[1].set_title(\"price in sek\")\n",
    "ax[2].boxplot(used_car_data[\"mileage_km\"])\n",
    "ax[2].set_title(\"mileage in km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some car advertisments did not have a price. These advertisments were given a price of 0 swedish krones. As these car advertisments are not insightful I removed them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_data.drop(\n",
    "    used_car_data.index[used_car_data[\"price_sek\"] <= 0], axis=0, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_whisker_bounds(data):\n",
    "    \"\"\"Calculate whisker bounds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Series\n",
    "        data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lower_bound: int\n",
    "        end of lower whisker\n",
    "    upper_bound: int\n",
    "        end of upper whisker\n",
    "    \"\"\"\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "\n",
    "    return (lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "def remove_outliers(dataset, category):\n",
    "    \"\"\"Remove outliers in given category.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : DataFrame\n",
    "        data set\n",
    "    category : str\n",
    "        category in which outliers shall be removed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataset : DataFrame\n",
    "        updated data set\n",
    "    \"\"\"\n",
    "    lower_bound, upper_bound = calculate_whisker_bounds(used_car_data[category])\n",
    "\n",
    "    index_out_of_range_entry_years = dataset.index[\n",
    "        (dataset[category] > upper_bound) | (dataset[category] < lower_bound)\n",
    "    ]\n",
    "\n",
    "    number_cars = dataset.shape[0]\n",
    "    dataset.drop(index_out_of_range_entry_years, axis=0, inplace=True)\n",
    "\n",
    "    print(\n",
    "        f\"number of used cars before and after removing outliers from {category}: \",\n",
    "        f\"{number_cars} / {dataset.shape[0]}\",\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_data = remove_outliers(used_car_data, \"entry_year\")\n",
    "used_car_data = remove_outliers(used_car_data, \"price_sek\")\n",
    "used_car_data = remove_outliers(used_car_data, \"mileage_km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10, 3))\n",
    "ax[0].boxplot(used_car_data[\"entry_year\"])\n",
    "ax[0].set_title(\"entry year\")\n",
    "ax[1].boxplot(used_car_data[\"price_sek\"])\n",
    "ax[1].set_title(\"price in sek\")\n",
    "ax[2].boxplot(used_car_data[\"mileage_km\"])\n",
    "ax[2].set_title(\"mileage in km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove any non necessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_data.drop(\n",
    "    [\n",
    "        \"note\",\n",
    "        \"engine_size_ccm\",\n",
    "        \"top_speed_km_h\",\n",
    "        \"length_mm\",\n",
    "        \"width_mm\",\n",
    "        \"height_mm\",\n",
    "        \"load_capacity_kg\",\n",
    "        \"empty_weight_kg\",\n",
    "        \"total_weight_kg\",\n",
    "        \"number_of_seats\",\n",
    "        \"url\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# remove cars which could not be classified by their type\n",
    "used_car_data.drop(\n",
    "    index=used_car_data.index[used_car_data[\"imputed_car_type\"].isna()], inplace=True\n",
    ")\n",
    "\n",
    "used_car_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save cleaned data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_data.reset_index(drop=True, inplace=True)\n",
    "used_car_data.to_csv(\"../data/cleaned_used_car_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('env-second-hand-car-analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fc0c6145edc3ecbf7a80eae99e7480605bf1eebe142aa02911632f34a024ef8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
